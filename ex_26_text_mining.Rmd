---
title: "Ch 27 Text Mining"
output:
  html_document:
    df_print: paged
    number_sections: true
    toc: true
    pandoc_args: [
      "--number-sections",
      "--number-offset=27"
    ]
---


## Exercises


>Project Gutenberg is a digital archive of public domain books. The R package __gutenbergr__ facilitates the importation of these texts into R.
>
>You can install and load it like this:
>```{r, eval=FALSE}
>install.packages("gutenbergr")
>library(gutenbergr)
>```
>
>You can see the books that are available like this:
>
>```{r, eval=FALSE}
>gutenberg_metadata
>```

```{r}
#install.packages("gutenbergr")
library(gutenbergr)
library(tidyverse)
```


>1. Use `str_detect` to find the ID of the novel _Pride and Prejudice_. 

Since the numeration in the gutenberg data starts with zero, we need to decrease the indexes by one.
Here, I use the function `str_which` that combines the commands `str_detect` and `which`.
```{r}
idx <- gutenberg_metadata %>% .$title %>% str_which("Pride and Prejudice") %>% .[]-1
```

In total we get 6 hits for the book "Pride and Prejudice".
```{r}
gutenberg_metadata %>% filter(gutenberg_id %in% idx)
```


>2. We notice that there are several versions. The `gutenberg_works()` function filters this table to remove replicates and include only English language works. Read the help file and use this function to find the ID for _Pride and Prejudice_

After removing the duplicates we end up with just two titles.
```{r}
gutenberg_works() %>% filter(gutenberg_id %in% idx)
```


>3. Use the `gutenberg_download` function to download the text for Pride and Prejudice. Save it to an object called `book`.

```{r}
book <- gutenberg_download(gutenberg_id = 1342)
str(book)
```

    
>4. Use the __tidytext__ package to create a tidy table with all the words in the text.

```{r}
library(tidytext)
book_tok <-book %>% unnest_tokens(word, text)
book_tok
```

   
>5. We will later make a plot of sentiment versus location in the book. For this, it will be useful to add a column with the word number to the table. 

```{r}
book_tok <- book_tok %>% mutate(word_number = row_number())
```

>6. Remove the stop words and numbers from the `stop_words` object. Hint: use the `anti_join`. 

```{r}
book_words <- anti_join(book_tok,stop_words, by="word")%>% filter(!word %in% c(1:100)) %>% arrange(word)
book_words 
```

   
>7. Now use the `AFINN` lexicon to assign a sentiment value to each word.

```{r}
afinn <- sentiments %>% filter(lexicon == "AFINN") %>% select(word, score)
book_score <- inner_join(book_words,afinn,by="word")
```

    
>8. Make a plot of sentiment score versus location in the book and add a smoother.

```{r}
book_score %>% ggplot(aes(word_number,score)) + geom_point()
```

```{r}
book_score %>% ggplot(aes(word_number,score)) + geom_smooth()
```


   
>9. Assume there are 300 words per page. Convert the locations to pages and then compute the average sentiment in each page. Plot that average score by page. Add a smoother that appears to go through data.

```{r}
book_page <- book_score %>% mutate(page_number = round(word_number/300))
```

```{r}
book_page %>% ggplot(aes(page_number,score)) + geom_smooth()
```

   